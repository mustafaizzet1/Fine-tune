# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1upCdPs_XozKGTYlTGThFBnE_DUMHbZmi
"""

pip install unsloth

pip install datasets trl bitsandbytes peft accelerate

from unsloth import FastLanguageModel

model_name = "unsloth/Llama-3.2-3B-Instruct"
max_seq_length = 2048
load_in_4bit = True  # T4 için ideal

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = model_name,
    max_seq_length = max_seq_length,
    dtype = None,
    load_in_4bit = load_in_4bit,
)



from datasets import load_dataset
dataset = load_dataset("json", data_files="/content/veri_unsloth.jsonl", split="train")

dataset[0]["messages"]
tokenizer.apply_chat_template(dataset[0]["messages"], tokenize=False)

model = FastLanguageModel.get_peft_model(
    model,
    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj",],
    lora_alpha = 16,
    lora_dropout = 0, # Supports any, but = 0 is optimized
    bias = "none",    # Supports any, but = "none" is optimized
    # [NEW] "unsloth" uses 30% less VRAM, fits 2x larger batch sizes!
    use_gradient_checkpointing = "unsloth", # True or "unsloth" for very long context
    random_state = 3407,
    use_rslora = False,  # We support rank stabilized LoRA
    loftq_config = None, # And LoftQ
)

from pprint import pprint
pprint(dataset[0])

from trl import SFTTrainer
from transformers import TrainingArguments, DataCollatorForSeq2Seq
from unsloth import is_bfloat16_supported
from unsloth.chat_templates import get_chat_template # Import get_chat_template here


def formatting_func(examples):
    texts = []
    messages_list = examples["messages"]

    for messages in messages_list:
        # Eğer yanlışlıkla tek bir mesaj dict'i gelirse, listeye sar
        if isinstance(messages, dict):
            messages = [messages]

        if not isinstance(messages, list):
            print("Geçersiz format atlandı:", messages)
            continue

        try:
            formatted = tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=False
            )
            texts.append(formatted)
        except Exception as e:
            print("Hatalı örnek atlandı:", e)
            continue

    return texts  # ✅ Sadece string listesi döndür!

trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = dataset,
    max_seq_length = max_seq_length,
    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),
    dataset_num_proc = 2,
    packing = False, # Can make training 5x faster for short sequences.
    formatting_func = formatting_func,
    args = TrainingArguments(
        per_device_train_batch_size = 2,
        gradient_accumulation_steps = 4,
        warmup_steps = 5,
        # num_train_epochs = 1, # Set this for 1 full training run.
        max_steps = 60,
        learning_rate = 2e-4,
        fp16 = not is_bfloat16_supported(),
        bf16 = is_bfloat16_supported(),
        logging_steps = 1,
        optim = "adamw_8bit",
        weight_decay = 0.01,
        lr_scheduler_type = "linear",
        seed = 3407,
        output_dir = "3B",
        report_to = "wandb", # Use this for WandB etc
         # Unsloth'un özel compute_loss'unu devre dışı bırakın - Bu satırı yorumdan çıkararak deneyin

    ),
)

print(dataset[0])
texts = formatting_func({"messages": dataset["messages"]})
print(len(texts), len(dataset))  # Bu iki sayı eşit olmalı

from unsloth.chat_templates import train_on_responses_only
trainer = train_on_responses_only(
    trainer,
    instruction_part = "<|start_header_id|>user<|end_header_id|>\n\n",
    response_part = "<|start_header_id|>assistant<|end_header_id|>\n\n",
)

tokenizer.decode(trainer.train_dataset[5]["input_ids"])

space = tokenizer(" ", add_special_tokens = False).input_ids[0]
tokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[5]["labels"]])

trainer_stats = trainer.train()

trainer.model.save_pretrained("outputs/final2_model")
tokenizer.save_pretrained("outputs/final2_model")

from transformers import pipeline

pipe = pipeline("text-generation", model=trainer.model, tokenizer=tokenizer)

prompt = tokenizer.apply_chat_template([
    {"role": "user", "content": "Bir marka tescili hangi şartlarda iptal edilir?"}
], tokenize=False, add_generation_prompt=True)

output = pipe(prompt, max_new_tokens=256)[0]["generated_text"]
print(output)

if False:
    from unsloth import FastLanguageModel
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name = "lora_model", # YOUR MODEL YOU USED FOR TRAINING
        max_seq_length = max_seq_length,
        dtype = dtype,
        load_in_4bit = load_in_4bit,
    )
    FastLanguageModel.for_inference(model) # Enable native 2x faster inference

messages = [
    {"role": "user", "content": "Bir şirket, piyasada uzun süredir var olan ve birçok firma tarafından üretilen bir tasarımı kendi adına tescil ettirmiştir. Bu durumda, söz konusu tasarımın yenilik ve ayırt edicilik özelliklerine sahip olmadığı gerekçesiyle hükümsüzlük davası açılabilir mi?"},
]
inputs = tokenizer.apply_chat_template(
    messages,
    tokenize = True,
    add_generation_prompt = True, # Must add for generation
    return_tensors = "pt",
).to("cuda")

from transformers import TextStreamer
text_streamer = TextStreamer(tokenizer, skip_prompt = True)
_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,
                   use_cache = True, temperature = 1.5, min_p = 0.1)